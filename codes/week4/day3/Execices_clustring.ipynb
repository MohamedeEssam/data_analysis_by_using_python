{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330d867d",
   "metadata": {},
   "source": [
    "#### <div class=\"alert alert-info\">Exercise 1 (plant clustering)</div>\n",
    "\n",
    "Using the same iris data set that you saw earlier in the classification, apply k-means clustering with 3 clusters.\n",
    "Create a function `plant_clustering` that loads the iris data set, clusters the data and returns the accuracy_score.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00efe1",
   "metadata": {},
   "source": [
    "#### <div class=\"alert alert-info\">Exercise 2 (nonconvex clusters)</div>\n",
    "\n",
    "This exercise can give four points at maximum!\n",
    "\n",
    "Read the tab separated file data.tsv from the `src` folder into a DataFrame. The dataset has two features X1 and X2, and the label y. Cluster the feature matrix using DBSCAN with different values for the eps parameter. Use values in `np.arange(0.05, 0.2, 0.05)` for clustering. For each clustering, collect the accuracy score, the number of clusters, and the number of outliers. Return these values in a DataFrame, where columns and column names are as in the below example.\n",
    "\n",
    "Note that DBSCAN uses label -1 to denote outliers , that is, those data points that didn't fit well in any cluster. You have to modify the find_permutation function to handle this: ignore the outlier data points from the accuracy score computation. In addition, if the number of clusters is not the same as the number of labels in the original data, set the accuracy score to NaN.\n",
    "\n",
    "         eps   Score  Clusters  Outliers                             \n",
    "    0    0.05      ?         ?         ?\n",
    "    1    0.10      ?         ?         ?\n",
    "    2    0.15      ?         ?         ?\n",
    "    3    0.20      ?         ?         ?\n",
    "\n",
    "Before submitting the solution, you can plot the data set (with clusters colored) to see what kind of data we are dealing with.\n",
    "\n",
    "Points are given for each correct column in the result DataFrame.\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb533f",
   "metadata": {},
   "source": [
    "#### <div class=\"alert alert-info\">Exercise 3 (binding sites)</div>\n",
    "\n",
    "This exercise can give three points at maximum!\n",
    "\n",
    "A binding site is a piece of DNA where a certain protein prefers to bind. The piece of DNA can be described as a string consisting of letters A, C, G, and T, which correspond to nucleotides Adenine, Cytosine, Guanine, and Thymine.\n",
    "In this exercise the length of binding sites is eight nucleotides. They are stored in the file `data.seq`,\n",
    "and the binding sites there are classified into two classes.\n",
    "\n",
    "Part 1. Write function `toint` that converts a nucleotide to an integer. Use the following mapping:\n",
    "```\n",
    "A -> 0\n",
    "C -> 1\n",
    "G -> 2\n",
    "T -> 3\n",
    "```\n",
    "\n",
    "Write also function `get_features_and_labels` that gets a filename as a parameter. The function should load the contents of the file into a DataFrame. The column `X` contains a string. Convert this column into a feature matrix using the above `toint` function. For example the column `[\"GGATAATA\",\"CGATAACC\"]` should result to the feature matrix\n",
    "```\n",
    "[[2,2,0,3,0,0,3,0],\n",
    "[1,2,0,3,0,0,1,1]]\n",
    "```\n",
    "The function should return a pair, whose first element is the feature matrix and the second element is the label vector.\n",
    "\n",
    "Part 2. Create function `cluster_euclidean` that gets a filename as parameter. Get the features and labels using the function from part 1. Perform hierarchical clustering using the function `sklearn.cluster.AgglomerativeClustering`. Get two clusters using `average` linkage and `euclidean` affinity. Fit the model and predict the labels. Note that you may have to use the `find_permutation` function again, because even though the clusters are correct, they may be labeled differently than the real labels given in `data.seq`. The function should return the accuracy score.\n",
    "\n",
    "Part 3. Create function `cluster_hamming` that works like the function in part 2, except now using the [hamming](https://en.wikipedia.org/wiki/Hamming_distance) affinity. Even though it is possible to pass the function `hamming` to `AgglomerativeClustering`, let us now compute the Hamming distance matrix explicitly. We can achieve this using the function `sklearn.metrics.pairwise_distances`. Use the affinity parameter `precomputed` to `AgglomerativeClustering`. And give the distance matrix you got from `pairwise_distances`, instead of the feature matrix, to the `fit_predict` method of the model. If you want, you can visualize the clustering using the provided `plot` function.\n",
    "\n",
    "**NB!** When submitting your solution, please comment away all `plot` function calls. This might cause tests to fail on the server.\n",
    "\n",
    "Which affinity (or distance) do you think is theoretically more correct of these two (Euclidean or Hamming)? Why?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
